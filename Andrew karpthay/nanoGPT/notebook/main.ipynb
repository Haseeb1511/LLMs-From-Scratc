{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b33e616a",
   "metadata": {},
   "source": [
    "# Decoder Only Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19920a",
   "metadata": {},
   "source": [
    "**B = batch size   the no of sequences(sentences) we process in parallel**\n",
    "\n",
    "**T = Time (sequence length / context length) Number of tokens in each sequence.  if T = 8, each input has 8 tokens (like 8 words/chars).**\n",
    "\n",
    "**V = vocab size (number of possible tokens)**\n",
    "\n",
    "**C = Dimensionality of the vector representing each token. Example: if C = 384, each token is mapped to a 384-dimensional embedding vector.**\n",
    "\n",
    "**The input to a Transformer is usually shaped:**\n",
    "**(B, T, C)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cde712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from math import log\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device  = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324df7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\",\"r\") as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d893e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char  = sorted(list(set(text)))\n",
    "vocab_size = len(char)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b030be",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 64\n",
    "block_size = 256 # Maximum context length for prediction?  no of token\n",
    "max_iter = 5000\n",
    "eval_iter = 200\n",
    "learning_rate = 3e-4\n",
    "eval_interval = 500\n",
    "n_embed = 384 # Embedding Dimensions\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9b6f8",
   "metadata": {},
   "source": [
    "### Creating a mapping for character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822fff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we are creating dictionary to which we gave character and it give index\n",
    "stoi = {ch:i for i,ch in enumerate(char)} #create dictionary with charater and its index--->{\"a\":1}\n",
    "# we are creating dictionary to which we gave index and it give the corresponding character\n",
    "itos = {i:ch for i,ch in enumerate(char)}  # create a dictionary with index and character -->{1:\"a\"}\n",
    "\n",
    "encode = lambda x : [stoi[c] for c in x]\n",
    "decode = lambda x :\"\".join([itos[i] for i in x])\n",
    "\n",
    "print(encode(\"How are you\"))\n",
    "print(decode(encode(\"How are you\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856fc3a",
   "metadata": {},
   "source": [
    "### Split the Data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67478f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text),dtype=torch.long)  # Encode the data\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]  # 90% data for training--># elements from start (index 0) up to n-1\n",
    "val_data = data[n:]  # last 10% for validation data---># elements from index n to the end\n",
    "\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f959532",
   "metadata": {},
   "source": [
    "#### Data batch for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4898a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "  data = train_data if split == \"train\" else val_data\n",
    "  ix = torch.randint(len(data)-block_size,(batch_size,)) # Pick random starting indices\n",
    "  # We used stack to get esult shape = (batch_size, block_size).\n",
    "  # Withput stack we get single dimension 1D tensor with stack\n",
    "  # Build inputs x\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix]) # input for model like ==> 47,10,30\n",
    "  # Build output y\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #output for model like ===> 10,30,\n",
    "  return x,y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xb,yb = get_batch(\"train\")\n",
    "print(\"Input:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"**\"*50)\n",
    "print(\"Outputs:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = xb[b,:t+1]\n",
    "    target = yb[b,t]\n",
    "    print(f\"When input is: {context.tolist()} the target is: {target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1154e87",
   "metadata": {},
   "source": [
    "# Self Attention Head(Single-Maske-Attention-Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c473845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.query_vector = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.key_vector = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value_vector = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.register_buffer(\"tril\",torch.tril(torch.ones(block_size,block_size)))\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        q = self.query_vector(x)\n",
    "        k = self.key_vector(x)\n",
    "        v = self.value_vector(x)\n",
    "        # Compute attention score\n",
    "        wei = q @ k.transpose(-2,-1)*C**-0.5  # scaling & c=dimesnion of vector  (B,T,C) @ (B,C,T).T==(B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0 ,float(\"-inf\")) #(B,T,T)\n",
    "        wei = F.softmax(wei,dim=-1) # (B,T,T)\n",
    "        # Dot product of attention score with the value vector\n",
    "        out = wei @ v  #(B,T,T) @ (B,T,C)===> (B,T,C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee1107",
   "metadata": {},
   "source": [
    "## Multi Head Attetntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class MultiHeadAttetnion(nn.Module):\n",
    "    \"\"\"\n",
    "Args:\n",
    "    num_heads (int): Number of parallel attention heads. Each head is its own self-attention block.\n",
    "        Multiple heads allow the model to capture different relationships or â€œrepresentation\n",
    "        subspacesâ€ of the input sequence at the same time.\n",
    "\n",
    "    head_size (int): Dimensionality of each attention headâ€™s output vector. Inside each head,\n",
    "        the input embedding is projected into query, key, and value vectors of size head_size.\n",
    "\n",
    "Example:\n",
    "    n_embed = 32\n",
    "    num_heads = 4\n",
    "    head_size = 8\n",
    "\n",
    "    - Each token embedding has 32 dimensions.\n",
    "    - Each head applies Linear(32 â†’ 8) to produce q, k, v vectors of size 8.\n",
    "    - One head outputs (B, T, 8). With 4 heads, we get 4 such outputs.\n",
    "    - Concatenating across heads gives (B, T, 32).\n",
    "\n",
    "Notes:\n",
    "    - Each head captures a different aspect of the tokenâ€™s meaning.\n",
    "    - Concatenation restores the original embedding size (32), ensuring consistent\n",
    "      dimensionality across layers.\n",
    "    - If num_heads * head_size â‰  n_embed, the multi-head output size will not match\n",
    "      the input size. In standard transformers, a final Linear projection is used\n",
    "      to resolve this mismatch.\n",
    "\n",
    "    embedding = 32\n",
    "    num_head = 4\n",
    "    head_output = 8\n",
    "    mean 4 head will have input of 32 dimension and converted to 8 dimesnion q,k,v vector and at the end we will have 4 output of 8 dimension we will concatenate it and get 2 dimesnion final output of multi head attention\n",
    "    if embedding size and final output dimension is not sam transformer will fail\n",
    "\n",
    "  \"\"\"\n",
    "    def __init__(self,num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # Each head learned independently, so you need a final mixing layer to let them interact. so we add a linear layer to let them interact\n",
    "        # It takes the concatenated output (B, T, n_embed) and projects it back into the embedding space.\n",
    "        # This way, the output of multi-head attention has the same dimension as the input embedding.\n",
    "        # This is necessary so you can:\n",
    "        # Add residual connections (x + attention_out)\n",
    "        self.proj = nn.Linear(n_embed,n_embed)  # Basically after concatination multiple head output we get bigger dimesnion matrix to make it same as input matrix we apply linear transformation to get same ammount of dimesnion as input\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads],dim=-1) # concatinating over the C Dimension    (B,T,C)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40cd3dc",
   "metadata": {},
   "source": [
    "## Feed-Forward_neuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aeb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed,4*n_embed),# First layer expands the embedding dimension (hidden_dim, usually n_embed).\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embed,n_embed), # Second layer projects back to n_embed so the shape matches the input (needed for residual connections).\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b9300",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,n_embed,n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_head   # 32/4==>8\n",
    "        self.sa = MultiHeadAttetnion(num_heads=n_head,head_size=head_size)\n",
    "        self.ffd = FeedForward(n_embed=n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)  #Layer normalize skip-connection\n",
    "        self.ln2 = nn.LayerNorm(n_embed)  #Layer normalize skip-connection\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Self-attention with residual connection\n",
    "        x = x + self.sa(self.ln1(x))   # we apply layer normalize directly on the input not like in transformer architecture\n",
    "        # Feed-forward with residual connection\n",
    "        x = x + self.ffd(self.ln2(x))  #residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537c614",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1260c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = batch size\n",
    "# T = sequence length (context length we fed in)\n",
    "# V = vocab size (number of possible tokens)\n",
    "class Bigramlanguagemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Step 1: Embedd the Token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
    "        # Step 2: Apply the positional Embedding same dimesnion as input embedding\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embed)  \n",
    "        # step 3: Masked Attention Blocked\n",
    "        self.block = nn.Sequential(*[Block(n_embed,n_head=n_head) for _ in range(n_layer)]) # how many layer of self attention we want\n",
    "\n",
    "        self.ln_F = nn.LayerNorm(n_embed)  #fianl layer norm\n",
    "        self.lm_head = nn.Linear(n_embed,vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self,idx,target=None): # idx is the input text, already tokenized and converted to integers from your vocabulary.\n",
    "        B,T = idx.shape\n",
    "        token_emb = self.token_embedding_table(idx)  # ==> (B,T,C)==>(4,8,vocab_size)==>torch.Size([4, 8, 65])\n",
    "        pos_embedding = self.position_embedding_table(torch.arange(T,device=device))  # (T,C)\n",
    "        x = token_emb + pos_embedding # Sum the postion embedding + token embedding  ==> (B,T,C)\n",
    "        \n",
    "        x = self.block(x)\n",
    "        logits = x=self.lm_head(x) # (B,T,vocab_szie)\n",
    "        loss=None\n",
    "        if target is not None:\n",
    "            #RuntimeError: Expected target size [4, 65], got [4, 8]\n",
    "            # loss function accept (B,C)\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits,target)\n",
    "        return logits,loss\n",
    "\n",
    "\n",
    "    def generate(self,idx,max_new_token):\n",
    "        # it take (B,T) and make it ==> (B,T+1,T+2...)\n",
    "        #idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_token):#get the predeiction\n",
    "            #crop idx to last block size token\n",
    "            idx_cond = idx[:,-block_size:]    \n",
    "            logits,loss = self(idx_cond)  # self is forward function\n",
    "            #focus on the last time step\n",
    "            # : â†’ keep all batches (B)\n",
    "            # -1 â†’ only take the last time step (T-1)\n",
    "            # : â†’ keep all vocab logits (C)\n",
    "            logits = logits[:,-1,:] # become (B,C)\n",
    "            #apply softmax to get probability\n",
    "            probs = F.softmax(logits,dim=-1) #(B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sample index to running samples\n",
    "            idx = torch.cat((idx,idx_next),dim=1) # (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bigramlanguagemodel() # Model object\n",
    "optimizer = torch.optim.AdamW(model.parameters(),learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac48e6",
   "metadata": {},
   "source": [
    "## Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06339348",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\",\"val\"]:\n",
    "        losses = torch.zeros(eval_iter)\n",
    "        for k in range(eval_iter):\n",
    "            x,y = get_batch(split) #get batch using get_batch function\n",
    "            logit,loss = model(x,y) #pass the batch to model function\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2f6cc0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iter):\n",
    "    if iter % eval_interval==0:   # iter % eval_interval â†’ the remainder when iter is divided by eval_interval.  0 mean \"no remainder\"\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}:train loss {losses[\"train\"]:.4f} val loss {losses[\"val\"]:.4f}\")\n",
    "\n",
    "        xb,yb = get_batch(\"train\")\n",
    "\n",
    "        optimizer.zero_grad()  #clear gradient accumulation\n",
    "        logits,loss = model(xb,yb)  #pass the input to model\n",
    "        loss.backward()#backpropogation\n",
    "        optimizer.step() #update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bb17d8",
   "metadata": {},
   "source": [
    "## Generate from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from the model\n",
    "context = torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "output = model.generate(idx=context, max_new_token=2000)\n",
    "decoded = decode(output[0].tolist())  # convert tensor â†’ list[int]\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef9976",
   "metadata": {},
   "source": [
    "# Mathematic trick in Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdb78e",
   "metadata": {},
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2  #Batch,Time Channel\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei/wei.sum(1,keepdim=True)\n",
    "xbow2 = wei @ x   # (T,T) @ (B,T,C)===pytorch will create (B,T,T) @ (B,T,C) ====> (B,T,C)\n",
    "wei.shape,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec80ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version  : SoftMax\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))  # Lower traingular metrics\n",
    "wei = torch.zeros((T,T)) # A zero metric of T row and T column===>> T=8\n",
    "\n",
    "wei = wei.masked_fill(tril==0,float(\"-inf\"))  # fill the lower triangular part with zero while the other part with '-inf' or where tril==0 make it -inf\n",
    "\n",
    "wei = F.softmax(wei,dim=-1)  #apply softmax on it == softmax(inf)=0\n",
    "#So dim=-1 ensures: \"for each query token, distribute its attention across all keys, summing to 1\".\n",
    "\n",
    "xbow3 = wei @ x  # (B,T,T) @ (B,T,C) ===> (B,T,C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ae79e",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a5dae",
   "metadata": {},
   "source": [
    "# ðŸ“ Encoder Block â€“ Cheatsheet\n",
    "\n",
    "### Self Attention\n",
    "\n",
    "* In **self attention** all 3 vectors (Query, Key, Value) come from the **same sequence**.\n",
    "* In **multi-head attention** we have multiple self-attention layers (heads) inside the block to capture **different meanings** of the input sequence.\n",
    "* In **cross attention** the Query comes from one sequence (decoder output) while Key and Value come from another sequence (encoder output).\n",
    "\n",
    "---\n",
    "\n",
    "### How Q, K, V are made\n",
    "\n",
    "* Query, Key, and Value vectors are generated by applying **linear transformation** on the embedding vector.\n",
    "* Usually done with a small neural network (linear layers) that learns the projection matrices.\n",
    "* Then dot product is applied to form attention scores.\n",
    "\n",
    "---\n",
    "\n",
    "### Flow of Encoder Block\n",
    "\n",
    "1. **Tokenize input**\n",
    "\n",
    "   * Convert words â†’ tokens â†’ numbers.\n",
    "\n",
    "2. **Embedding vector**\n",
    "\n",
    "   * Convert token IDs â†’ dense embedding vectors.\n",
    "\n",
    "3. **Positional encoding**\n",
    "\n",
    "   * Create positional encoding (same dimension as embedding).\n",
    "   * Add embedding + positional encoding â†’ gives input with position info.\n",
    "\n",
    "4. **Self Attention layer**\n",
    "\n",
    "   * Generate Q, K, V vectors using linear transformation.\n",
    "   * Compute dot product: ( w = Q \\times K^T ).\n",
    "   * Normalize: divide by (\\sqrt{\\text{dimension of K}}).\n",
    "   * Apply softmax â†’ get normalized weights in range 0â€“1.\n",
    "   * Multiply with Value vector: ( y = w \\times V ).\n",
    "\n",
    "5. **Feed Forward Neural Network (FFN)**\n",
    "\n",
    "   * Pass the result into a neural network.\n",
    "   * Apply **ReLU activation** to add non-linearity.\n",
    "   * Output layer of the FFN has same dimension as embedding vector.\n",
    "\n",
    "6. **Final Output**\n",
    "\n",
    "   * Encoder output = **contextual embedding** of each token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43b820",
   "metadata": {},
   "source": [
    "# ðŸ“ Decoder Block â€“ Cheatsheet\n",
    "\n",
    "### Cross vs Self Attention\n",
    "\n",
    "* **Decoder has 2 attention layers**:\n",
    "\n",
    "  1. **Masked Self Attention** â†’ lets decoder look at past tokens only no futer token done by ==> wei.masked_fill(tril==0,float(\"-inf\")) .\n",
    "  2. **Cross Attention** â†’ Query comes from decoder, Key + Value come from encoder output.\n",
    "\n",
    "---\n",
    "\n",
    "### Flow of Decoder Block\n",
    "\n",
    "1. **Tokenize target sequence**\n",
    "\n",
    "   * Convert target sentence (shifted right during training to add <SOS>) â†’ tokens â†’ numbers.\n",
    "\n",
    "2. **Embedding vector**\n",
    "\n",
    "   * Convert token IDs â†’ dense embedding vectors.\n",
    "\n",
    "3. **Positional encoding**\n",
    "\n",
    "   * Add positional encoding to embeddings(same dimension as input embedding).\n",
    "\n",
    "4. **Masked Self Attention**\n",
    "\n",
    "   * Generate Q, K, V from decoder input (like encoder).\n",
    "   * Apply mask (future tokens hidden) ==> wei.masked_fill(tril==0,float(\"-inf\")).\n",
    "   * Compute attention: ( w = softmax(Q.K.T/sqrt(Dimesnion of K))) . V\n",
    "   * This ensures each position only attends to **previous tokens**.\n",
    "\n",
    "5. **Cross Attention**\n",
    "\n",
    "   * Query (Q) from decoder hidden states.\n",
    "   * Key (K), Value (V) from encoder output.\n",
    "   * Attention = how decoder tokens attend to encoder tokens.\n",
    "   * Output is contextual info combining encoder + decoder states.\n",
    "\n",
    "6. **Feed Forward Neural Network (FFN)**\n",
    "\n",
    "   * Pass attention output into FFN with ReLU for non-linearity.\n",
    "   * Output dimension = same as embedding size.\n",
    "\n",
    "7. **Final Softmax Layer (Prediction)**\n",
    "\n",
    "   * After stacking decoder blocks, final layer projects output â†’ vocabulary size.\n",
    "   * Apply softmax â†’ gives probability distribution of next token.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4 : Self Attention\n",
    "# If We want Encoder Block only  we remove the masking (wei.masked_fill(tril==0,float(\"-inf\")))\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)  # Dummy user input\n",
    "\n",
    "head_size = 16  #no of Mult-Head Attention (to capture differnt meaning of sentence)\n",
    "query_vector = nn.Linear(C,head_size,bias=False)\n",
    "key_vector = nn.Linear(C,head_size,bias=False)\n",
    "value_vector = nn.Linear(C,head_size,bias=False)\n",
    "q = query_vector(x)  #(B,T,16)\n",
    "k = key_vector(x)  # (B,T,16)\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T)===>(B,T,T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))  # Lower traingular metrics\n",
    "wei = wei.masked_fill(tril==0,float(\"-inf\"))  # fill the lower triangular part with zero while the other part with '-inf' or where tril==0 make it -inf\n",
    "wei = F.softmax(wei,dim=-1)  #apply softmax on it == softmax(inf)=0  # So dim=-1 ensures: \"for each query token, distribute its attention across all keys, summing to 1\".\n",
    "v = value_vector(x)\n",
    "out = wei @ v  # multiplt softmax output with value vector ==(B,T,head_size)\n",
    "out.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409e720",
   "metadata": {},
   "source": [
    "# Why we scaled the softmax values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf7613",
   "metadata": {},
   "source": [
    "1. For Larger dimesnion the output do dot product of q @ k is high and variance become high\n",
    "2. For lower dimension vector the output do dot product of q @ k is low and variance become low\n",
    "3. And after applying softmax the value with higher variance get more probability compare to samlle variance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0caf045",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(B,T,head_size)\n",
    "k = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without scaling variance is very high and not preserved\n",
    "k.var(),q.var(),wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b92e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After applying scaling\n",
    "q = torch.randn(B,T,head_size)\n",
    "k = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1)*head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After scaling variance is preserved\n",
    "k.var(),q.var(),wei.var()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

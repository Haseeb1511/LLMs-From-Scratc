{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEtlR-nQBW3p",
        "outputId": "f7e2680d-8706-4f52-9920-471ea00fab9b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sentencepiece as spm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy8OGWnI4YKu",
        "outputId": "be859ff0-f538-496f-fa39-d60e8f178786"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1TrsUE78zvz",
        "outputId": "2f8ce508-633f-422c-9634-1c5f14c4b046"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZRvgyGa88A1",
        "outputId": "901bd369-1a07-4cec-a106-1ca64504b8d4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# copy the file back to colab from drive\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/MachineTranslation\"\n",
        "\n",
        "# List files\n",
        "!ls \"{save_dir}\"\n",
        "\n",
        "# Copy files to Colab\n",
        "!cp \"{save_dir}/checkpoint_epoch_12.pt\" /content/\n",
        "!cp \"{save_dir}/metrics.json\" /content/\n",
        "!cp \"{save_dir}/urdu.model\" /content/\n",
        "!cp \"{save_dir}/english.model\" /content/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulNBIshhC_JN"
      },
      "source": [
        "- Each SentencePiece model built different subword rules.\n",
        "\n",
        "- So what used to be 200 tokens with tokenizer A might become 600 tokens with tokenizer B.\n",
        "- SentencePiece is a subword tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "J7EtyjCkgocu",
        "outputId": "00eb64ed-333b-4d4f-a70e-740a62191ac0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"merge_data.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-g8REsmBToJ",
        "outputId": "54812006-0a0a-46d0-b202-e6dcd3febc8e"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpPwVdC0gmOo"
      },
      "outputs": [],
      "source": [
        "with open(\"english.txt\" ,\"w\",encoding=\"utf-8\") as f:\n",
        "  for english in df[\"input\"]:\n",
        "    f.write(str(english) + \"\\n\")\n",
        "\n",
        "with open(\"urdu.txt\" ,\"w\",encoding=\"utf-8\") as f:\n",
        "  for urdu in df[\"target\"]:\n",
        "    f.write(str(urdu) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxQKb-1-SuBP"
      },
      "source": [
        "# Tokenizer\n",
        "- when we tokenize we need to combine both english urdu into single txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWHwagd2UMtF"
      },
      "outputs": [],
      "source": [
        "# # Englis Tokenizer\n",
        "# spm.SentencePieceTrainer.train(\n",
        "#     input=\"english.txt\",\n",
        "#     model_prefix=\"english\",\n",
        "#     vocab_size=8000,\n",
        "#     model_type=\"bpe\",\n",
        "#     bos_id=1,  # Add BOS token\n",
        "#     eos_id=2,  # Add EOS token\n",
        "#     unk_id=0,\n",
        "#     pad_id=3\n",
        "# )\n",
        "\n",
        "# spm.SentencePieceTrainer.train(\n",
        "#     input=\"urdu.txt\",\n",
        "#     model_prefix=\"urdu\",\n",
        "#     vocab_size=8000,\n",
        "#     model_type=\"bpe\",\n",
        "#     bos_id=1,\n",
        "#     eos_id=2,\n",
        "#     unk_id=0,\n",
        "#     pad_id=3\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1am759NFYICG",
        "outputId": "f628c10f-2a9e-4a5e-9b7f-a96f38636575"
      },
      "outputs": [],
      "source": [
        "# Reload tokenizers\n",
        "sp_en = spm.SentencePieceProcessor()\n",
        "sp_en.load(\"english.model\")\n",
        "\n",
        "sp_ur = spm.SentencePieceProcessor()\n",
        "sp_ur.load(\"urdu.model\")\n",
        "\n",
        "# Verify special tokens\n",
        "print(f\"English - BOS: {sp_en.bos_id()}, EOS: {sp_en.eos_id()}, UNK: {sp_en.unk_id()}, PAD: {sp_en.pad_id()}\")\n",
        "print(f\"Urdu - BOS: {sp_ur.bos_id()}, EOS: {sp_ur.eos_id()}, UNK: {sp_ur.unk_id()}, PAD: {sp_ur.pad_id()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4S3AYsuCIeT",
        "outputId": "a00604a5-4300-4c6b-d51c-a8e9e841b1ef"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 256\n",
        "df[\"en_len\"] = df[\"input\"].apply(lambda x: len(sp_en.encode(str(x))))\n",
        "df[\"ur_len\"] = df[\"target\"].apply(lambda x: len(sp_ur.encode(str(x))))\n",
        "\n",
        "df_clean = df[(df['en_len'] <= MAX_LEN) & (df['ur_len'] <= MAX_LEN)]  # remove sequen6ce greater than 256 tokens\n",
        "print(f\"Original data: {len(df)} samples\")\n",
        "print(f\"Filtered data: {len(df_clean)} samples\")\n",
        "print(f\"Removed: {len(df) - len(df_clean)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcCrBGe_Hjc3"
      },
      "source": [
        "# Convert Dataset → Token IDs with Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GekZpTwEwBeq",
        "outputId": "0002c187-360b-47aa-9852-f2e410186dc7"
      },
      "outputs": [],
      "source": [
        "english_sentences = df_clean['input'].tolist()\n",
        "urdu_sentences = df_clean['target'].tolist()\n",
        "\n",
        "# Compute max length for each\n",
        "max_len_en = max(len(sp_en.encode(sent)) for sent in english_sentences if isinstance(sent, str))\n",
        "max_len_ur = max(len(sp_ur.encode(sent)) for sent in urdu_sentences if isinstance(sent, str))\n",
        "\n",
        "print(\"Max English sequence length:\", max_len_en)\n",
        "print(\"Max Urdu sequence length:\", max_len_ur)\n",
        "\n",
        "max_len = max(max_len_en, max_len_ur)\n",
        "print(\"Overall max sequence length:\", max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da4jf_S8Pq08",
        "outputId": "75ce4e31-3629-4387-dc42-fd0cd17245d7"
      },
      "outputs": [],
      "source": [
        "print(sp_en.get_piece_size(), sp_ur.get_piece_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGuBC27KuzJj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_input, val_input, train_target, val_target = train_test_split(\n",
        "    df[\"input\"], df[\"target\"], test_size=0.1, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfUodeOyHjHX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "max_len = max_len\n",
        "\n",
        "bos_id = sp_ur.piece_to_id(\"<BOS>\")   # it get BOS id in SentencePiece and return its ID which is 1\n",
        "eos_id = sp_ur.piece_to_id(\"<EOS>\")  # it get EOS id in SentencePiece and return its ID which is 2\n",
        "\n",
        "def encode_sentence(example, sp, max_len, add_bos_eos=False):\n",
        "    \"\"\"\n",
        "    add_bos_eos = if True, BOS (1) and EOS (2) will be added to all the tokens\n",
        "    \"\"\"\n",
        "    tokens = sp.encode(str(example), out_type=int)\n",
        "    if add_bos_eos:\n",
        "        bos_id = sp.bos_id()\n",
        "        eos_id = sp.eos_id()\n",
        "        tokens = [bos_id] + tokens + [eos_id]\n",
        "    return torch.tensor(tokens[:max_len], dtype=torch.long)   # [:max_len] ==> truncate the sequence so it doesn’t exceed the maximum length allowed\n",
        "\n",
        "\n",
        "\n",
        "# encode the text\n",
        "train_src = [encode_sentence(en, sp_en, max_len, add_bos_eos=False) for en in train_input]\n",
        "train_tgt = [encode_sentence(urdu, sp_ur, max_len, add_bos_eos=True) for urdu in train_target]\n",
        "\n",
        "val_src = [encode_sentence(en, sp_en, max_len, add_bos_eos=False) for en in val_input]\n",
        "val_tgt = [encode_sentence(urdu, sp_ur, max_len, add_bos_eos=True) for urdu in val_target]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1ji4Vm86RCLS",
        "outputId": "dcb603a0-2b8d-40f9-bfeb-cea6ab6ecaa8"
      },
      "outputs": [],
      "source": [
        "x = train_src[1].tolist() # sentence trnsformer expect a Python list of integers, not a PyTorch tensor.\n",
        "sp_ur.decode(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-an-OEJOQiAd"
      },
      "outputs": [],
      "source": [
        "# dataset returns variable-length torch.tensors so we us the below function to make them of same lenght\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    src_padded = [\n",
        "      [w1, w2, w3],\n",
        "      [w1, w2, PAD]]\n",
        "\n",
        "    src_mask = [\n",
        "      [1, 1, 1],  # all real tokens\n",
        "      [1, 1, 0]]   # 0 = PAD token\n",
        "\n",
        "    it returns:\n",
        "    input padded + masked\n",
        "    target padded + masked\n",
        "    \"\"\"\n",
        "    # batch = list of (src, tgt) pairs\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    pad_token_id = sp_ur.pad_id()  # use real pad id\n",
        "\n",
        "    # Pad only to max length of this batch with correct pad id\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_token_id)  # we make sequnce exuall length by adding ==>  PAD (special token)\n",
        "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=pad_token_id)\n",
        "\n",
        "    # Create masks using pad_token_id (1=real, 0=pad)\n",
        "      # we check in sec_padded element wise if token equall  0 than  it return True if token is 0 and it  return False if token is not 0\n",
        "    # Then we convert true false to@\n",
        "    # True → 1 (real token)\n",
        "    # False → 0 (padding token\n",
        "    src_mask = (src_padded != pad_token_id).long()\n",
        "    tgt_mask = (tgt_padded != pad_token_id).long()\n",
        "\n",
        "    return src_padded, tgt_padded, src_mask, tgt_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF51g46swHpE"
      },
      "source": [
        "# Dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXpwWLybwKJC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src, tgt):\n",
        "        self.src = src\n",
        "        self.tgt = tgt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src[idx], self.tgt[idx]  # it returns a tuple\n",
        "\n",
        "\n",
        "train_dataset = TranslationDataset(train_src, train_tgt)\n",
        "val_dataset   = TranslationDataset(val_src, val_tgt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwfH_hEEQuKr",
        "outputId": "b3d504b0-3ebe-4d78-e7ed-b2d2a190dd82"
      },
      "outputs": [],
      "source": [
        "eng,urd = train_dataset[1]\n",
        "print(eng,urd)\n",
        "print(sp_en.decode(eng.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2kN6zMFTPK5",
        "outputId": "8af7cd0e-ad5b-483b-afa7-a3a62e04b91f"
      },
      "outputs": [],
      "source": [
        "batch = [train_dataset[0], train_dataset[1], train_dataset[2]]\n",
        "src_padded, tgt_padded, src_mask, tgt_mask = collate_fn(batch)\n",
        "print(f\"{'='*50}Pad_id = 3 {'='*50}\")\n",
        "print(\"Input(enlgish) padded\",src_padded)  # pad_id = 3\n",
        "print(f\"{'='*50}Pad_id = 3 {'='*50}\")\n",
        "print(\"Target(urdu) padded: \",tgt_padded)\n",
        "print(f\"{'='*50} Masked= 0, orignla = 1 {'='*50}\")\n",
        "print(\"Input Masked(English): \",src_mask)\n",
        "print(f\"{'='*50} Masked= 0 & original = 1 {'='*50}\")\n",
        "print(\"Target(Urdu) masked : \",tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTJW4UVR3tFD"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngOZ4HgTFaSF"
      },
      "source": [
        "# Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T73pus_qFbVs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class SinusoidalPostionalEncoding(nn.Module):\n",
        "    def __init__(self,max_len,d_model):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            max_len (int): maximum sequence length\n",
        "            d_model (int): embedding dimension\n",
        "        PE(pos, 2i)   = sin(pos / (10000^(2i/dim)))\n",
        "        PE(pos, 2i+1) = cos(pos / (10000^(2i/dim)))\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Step 1: Create a matrix to hold positional encodings\n",
        "        pe = torch.zeros(max_len,d_model) # Create a matrix of shape (max_len,d_model) to store positional encoding\n",
        "\n",
        "        ## Step 2: Create position indices (0 to max_len-1)\n",
        "        position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        ## Step 3: Compute inverse frequencies\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:,0::2] = torch.sin(position * div_term)  # even term for sine\n",
        "        pe[:,1::2] = torch.cos(position * div_term)  # odd term for cos\n",
        "\n",
        "        # Step 5: Add batch dimension for broadcasting\n",
        "        pe = pe.unsqueeze(0)  # shape: [1, max_len, dim]\n",
        "\n",
        "        # Register buffer so it's not a parameter (not trainable)\n",
        "        self.register_buffer(\"pe\",pe)  # (1,max_len,d_model)   # 1 is batch_dim placeholder for later brodcasting\n",
        "\n",
        "    def forward(self,x):\n",
        "      B,T,C = x.shape\n",
        "      # Add positional encoding to embeddings\n",
        "      return x + self.pe[:, :T, :]   # add x(input) + positional emnedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK_QyJG-dG44"
      },
      "outputs": [],
      "source": [
        "# max_len = 100\n",
        "# d_model = 64\n",
        "\n",
        "# pos_enc = SinusoidalPostionalEncoding(max_len, d_model)\n",
        "# # Dummy input (B,T,C)\n",
        "# x = torch.zeros(1, max_len, d_model)\n",
        "# pos_encoding = pos_enc(x).cpu().detach()  # (T,C)\n",
        "\n",
        "# # Plot\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.imshow(pos_encoding.squeeze(0), cmap='viridis') #(T,C)\n",
        "# plt.xlabel(\"Embedding Dimension\")\n",
        "# plt.ylabel(\"Position\")\n",
        "# plt.title(\"Sinusoidal Positional Encoding\")\n",
        "# plt.colorbar()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFq2s97JDm8z"
      },
      "source": [
        "# CrossAttention + SelfAttetnion  with Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydaaci0CDwJP"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  self atttenion with 3 features:\n",
        "  - cross attetnion\n",
        "  - multi head attetnion\n",
        "  - multi head attetnion with masking\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,d_model,n_heads,dropout,trace_shapes):\n",
        "    super().__init__()\n",
        "    assert d_model % n_heads==0,\"Error, Embedding dimension must be divisible with head dimension try chosing differnt head no\"\n",
        "    self.d_model = d_model  # Embedding dimesnion\n",
        "    self.n_head = n_heads  # No of Heads\n",
        "    self.d_head = d_model // n_heads  # Dimension of each Head\n",
        "\n",
        "    self.W_q = nn.Linear(d_model,d_model)  # Weight matrix of Query\n",
        "    self.W_k = nn.Linear(d_model,d_model)  # Weight matrix of Key\n",
        "    self.W_v = nn.Linear(d_model,d_model)   # Weight matrix of Value\n",
        "    self.proj = nn.Linear(d_model,d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.trace_shapes = trace_shapes  # to print shapes\n",
        "\n",
        "  def forward(self,x,enc_out,mask=None,padding_mask=None):\n",
        "    \"\"\"\n",
        "    - Self-attention: pass x only\n",
        "    - Cross-attention: pass q, k, v\n",
        "    - mask attetnion: futures token are masked\n",
        "    \"\"\"\n",
        "    # step 0 : Exptrant shape of encoder output and decoder input\n",
        "    B,T,C = x.shape\n",
        "    ## S stands for the sequence length of the encoder output — i.e., how many tokens the encoder processed.(B, S, C)\n",
        "    S = enc_out.shape[1] if enc_out is not None else T\n",
        "\n",
        "\n",
        "    # step 1: genertae query, key and value vector using the above linear layers\n",
        "    if enc_out is None:\n",
        "      # self attetnion\n",
        "      Q = self.W_q(x)  # Generate query vector  (B,T,d_model)\n",
        "      K = self.W_k(x)  # Generate key vector  (B,T,d_model)\n",
        "      V = self.W_v(x)  # Generate value vector  (B,T,d_model)\n",
        "    else:\n",
        "      # cross-attetnion\n",
        "        Q = self.W_q(x)          # (B, T, d_model) query from decoder hidden state\n",
        "        K = self.W_k(enc_out)    # (B, S, d_model) key from encoder output\n",
        "        V = self.W_v(enc_out)    # (B, S, d_model) value from encoder output\n",
        "\n",
        "\n",
        "    ## step 2 : reshape and split into multipe heads\n",
        "    if enc_out is None:\n",
        "      # Reshape\n",
        "      q = Q.view(B,T,self.n_head,self.d_head)   # (B,T,d_model) ==> (B,T,H,d_head)\n",
        "      k = K.view(B,T,self.n_head,self.d_head)   # (B,T,d_model) ==> (B,T,H,d_head)\n",
        "      v = V.view(B,T,self.n_head,self.d_head)   # (B,T,d_model) ==> (B,T,H,d_head)\n",
        "    else:\n",
        "      q = Q.view(B, T, self.n_head, self.d_head) # (B,T,d_model) ==> (B,T,H,d_head)  it is from decoder hidden state\n",
        "      k = K.view(B, S, self.n_head, self.d_head) # (B,S,H,d_head)\n",
        "      v = V.view(B, S, self.n_head, self.d_head) # (B,S,H,d_head)\n",
        "\n",
        "    # split in to muliple heads\n",
        "    q = q.transpose(1,2)  # (B,T,H,d_head)  ==> (B,H,T,d_head)\n",
        "    k = k.transpose(1,2)  # (B,T,H,d_head)  ==> (B,H,T,d_head)\n",
        "    v = v.transpose(1,2)  # (B,T,H,d_head)  ==> (B,H,T,d_head)\n",
        "\n",
        "\n",
        "\n",
        "    # Step 3 : Attetnion score calculation\n",
        "    # If no masking this will be att_score\n",
        "    att_score = (q @ k.transpose(-2,-1)) / (self.d_head**0.5) # att_score: (B,H,T,T) for self-attention, (B,H,T,S) for cross-attention\n",
        "\n",
        "\n",
        "    # print(\"att_score shape:\", att_score.shape)\n",
        "    # print(\"padding_mask shape:\", padding_mask.shape if padding_mask is not None else None)\n",
        "\n",
        "\n",
        "    # step 4: Padding and Masking\n",
        "    # Apply Padding to both Encoder and Decoder\n",
        "    if padding_mask is not None:\n",
        "      padding_mask = padding_mask.to(att_score.device)\n",
        "      # att_score has shape (B, H, T, S) (for cross-attention) or (B, H, T, T) (for self-attention)\n",
        "      # padding_mask usually has shape (B, S) (1 = keep, 0 = pad)\n",
        "      # att_score have 4-dimesnion so to make padding broadcastable for att_score we add 2-Dimesnion\n",
        "      # (B, S) -> (B, 1, 1, S) we add 2 Dimesnion\n",
        "      padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)  # (B,S) ==> (B,1,1,S)  for cross attention\n",
        "      att_score = att_score.masked_fill(padding_mask == 0, float('-inf'))\n",
        "\n",
        "\n",
        "    # Step 5 :Apply causal mask (only for decoder self-attention)\n",
        "    if mask is not None and enc_out is None:\n",
        "      casual_mask = torch.tril(torch.ones(T,T,device=x.device,dtype=torch.bool)).view(1,1,T,T)\n",
        "      att_score = att_score.masked_fill(casual_mask==0,float('-inf'))\n",
        "\n",
        "\n",
        "    # step 4 :weighted sum with values\n",
        "    att_weight = F.softmax(att_score,dim=-1)\n",
        "    att_weight = self.dropout(att_weight)  # applied dropout\n",
        "    out = att_weight @ v  ## (B,H,T,d_head)\n",
        "\n",
        "\n",
        "    # step 5 : concatenate the each Heads output\n",
        "    concatentaed_output = out.transpose(1,2)   # (B,H,T,h_dim) ==> (B,T,H,d_head)\n",
        "    # This ensures the tensor is stored contiguously in memory after a transpose. as transpose break memory allocation in pytorch and view can not work if tensor is not contigious.\n",
        "    concatentaed_output = concatentaed_output.contiguous()  #(B,T,H,d_head)\n",
        "    # C = H * head_dim  .view simply muliply the last 2 dimension to make them one dimesnion(C)\n",
        "    concatentaed_output = concatentaed_output.view(B,T,C)\n",
        "\n",
        "\n",
        "    # step 5: final layer projection\n",
        "    out = self.proj(concatentaed_output)  ## (B, T, d_model)\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI8fdvRCFc1d"
      },
      "source": [
        "# LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQqhA01_FeKa"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  \"\"\"\n",
        "  RMSNorm(Root Mean Square LayerNormalization)\n",
        "  y = x/RMS(X)  where RMS  is RMS = sqrt(eps + mean(x**2))\n",
        "  \"\"\"\n",
        "  def __init__(self,dim,eps=1e-12):\n",
        "    super().__init__()\n",
        "    self.eps =eps\n",
        "    self.weight = nn.Parameter(torch.ones(dim))  # trainable parameter(gamma)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Compute RMS over the last dimension\n",
        "    # As we take mean over last dimesion after mean it is removed so we keepdim=True so dimesion is not removed after mean\n",
        "    rms = torch.sqrt(torch.mean(x**2,dim=-1,keepdim=True) + self.eps)\n",
        "    rms_norm = x / rms\n",
        "    return self.weight * rms_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBkM2z5DerlI"
      },
      "source": [
        "## Activation function(SWiGLU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_0NL__5msQp"
      },
      "outputs": [],
      "source": [
        "class SwiGLU(nn.Module):\n",
        "  \"\"\"\n",
        "\tSwiGLU Activation Block\n",
        "\tFormula:\n",
        "\t  swish(x) = x * sigmoid(B * x)\n",
        "\t  SwiGLU(x) = (xW1) * swish(xW2)\n",
        "\t  output = Dropout( (xW1 * swish(xW2)) W3 )\n",
        "\n",
        "\t  dim = dimension of embedding\n",
        "\t  mult = multiply\n",
        "\t  inner = hidden layer\n",
        "\n",
        "\t  x ──► W1 ─────────────|\n",
        "\t\t\t\t\t\t\telement-wise multiply ──► W3 ──► output\n",
        "\t  x ──► W2 ──► Swish ──┘\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,dim,dropout=0.2,mult=4):\n",
        "    super().__init__()\n",
        "    inner = dim * mult\n",
        "    self.w1 = nn.Linear(dim,inner,bias=False)  # Main path\n",
        "    self.w2 = nn.Linear(dim,inner,bias=False)  # gate path\n",
        "    self.w3 = nn.Linear(inner,dim,bias=False)  # projection back to orignal dim\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.swish = nn.SiLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x1 = self.w1(x)   ## (batch, seq, inner)\n",
        "    x2 = self.swish(self.w2(x))\n",
        "    gate_output = x1 * x2  # element-wise gated combination\n",
        "    out = self.w3(gate_output)\n",
        "    return self.dropout(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvdTrdzipeZW"
      },
      "source": [
        "# FeedForwardNeuralNetwork(FFN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h59GA8LphYI"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    inner = 4*d_model\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(d_model,inner),    # expand the input dimesnion\n",
        "        nn.GELU(),  # to apply non linearity\n",
        "        nn.Linear(inner,d_model)  # project back to orignal dimesnion\n",
        "                            )\n",
        "  def forward(self,x):\n",
        "    return self.network(x)\n",
        "\n",
        "# class FFN(nn.Module):\n",
        "#   def __init__(self,d_model):\n",
        "#     super().__init__()\n",
        "#     inner = 4*d_model\n",
        "#     self.network = nn.Sequential(SwiGLU(d_model))\n",
        "\n",
        "#   def forward(self,x):\n",
        "#     return self.network(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK1uQUmqFfgA"
      },
      "source": [
        "# Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjG0h0H7Fild",
        "outputId": "3eaeba7c-06fd-4d87-bcbd-9bf805c6f206"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,dropout,trace_shapes=False):\n",
        "    super().__init__()\n",
        "    self.multi_head_att = MultiHeadAttention(d_model,n_heads,dropout,trace_shapes) #Multi head attetnion\n",
        "    self.ffn = FFN(d_model)\n",
        "    self.norm1 = LayerNorm(d_model)\n",
        "    self.norm2 = LayerNorm(d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x,enc_padding_mask=None,mask=None):\n",
        "    # Pre-Norm + addtition + dropout\n",
        "    # step 1: Multi head attetnion\n",
        "    attn_out = self.multi_head_att(self.norm1(x),\n",
        "                                   enc_out=None,\n",
        "                                   mask=mask,\n",
        "                                   padding_mask=enc_padding_mask)  # apply layer norm before applying multi head attetnion\n",
        "    x = x + self.dropout(attn_out)\n",
        "\n",
        "    # Step 2 FFNN\n",
        "    ffn_out = self.ffn(self.norm2(x))\n",
        "    x = x + self.dropout(ffn_out)\n",
        "    return x\n",
        "\n",
        "# test it\n",
        "n_heads = 4\n",
        "d_model = 12\n",
        "dropout=0.2\n",
        "padding_mask = torch.tensor([[1, 1, 0, 0]])  # last 2 positions are padding\n",
        "x = torch.rand(1,4,d_model)  #(B,T,C)\n",
        "encoder = Encoder(d_model,n_heads,dropout)\n",
        "out = encoder(x,padding_mask)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N2uOcmL1iKK"
      },
      "source": [
        "# Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCZaVWz3vO-4",
        "outputId": "e2b36386-daae-4d01-f665-04285aad81a1"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,dropout,trace_shapes=False):\n",
        "    super().__init__()\n",
        "    self.multi_head_att = MultiHeadAttention(d_model,n_heads,dropout,trace_shapes) #Multi head attetnion\n",
        "    self.cross_att = MultiHeadAttention(d_model, n_heads, dropout, trace_shapes) # cross attention get q from decoder hidden state and k,v from encoder output\n",
        "    self.ffn = FFN(d_model)\n",
        "    self.norm1 = LayerNorm(d_model)\n",
        "    self.norm2 = LayerNorm(d_model)\n",
        "    self.norm3 = LayerNorm(d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x,enc_out,tgt_padding_mask=None,enc_padding_mask=None,mask=None):\n",
        "      \"\"\"\n",
        "        x: decoder input (B, T , C)\n",
        "        enc_out: encoder output (B, T, C)\n",
        "\n",
        "      Cross-attention uses:\n",
        "        query (Q) → from decoder’s hidden state\n",
        "        key (K) and value (V) → from encoder output\n",
        "      \"\"\"\n",
        "      # Step 1: Self-attention (mask future tokens + padding)\n",
        "      att_out = self.multi_head_att(self.norm1(x),\n",
        "                                    enc_out=None,\n",
        "                                    mask=mask,\n",
        "                                    padding_mask=tgt_padding_mask)  # (B,T,C)  we pass tgt_padding_mask\n",
        "      x = x + self.dropout(att_out)  # multi head attention with masking\n",
        "\n",
        "\n",
        "      #step 2: Cross-Attention (queries from decoder, keys/values from encoder)(cross attention use encoder padding mask)\n",
        "      cross_att = self.cross_att(self.norm2(x),\n",
        "                                 enc_out,\n",
        "                                 mask=None,\n",
        "                                 padding_mask=enc_padding_mask)  #we pass enc_padding_mask\n",
        "      x = x + self.dropout(cross_att)\n",
        "\n",
        "      # step 3: ffn\n",
        "      ffn_out = self.ffn(self.norm3(x))\n",
        "      x = x + self.dropout(ffn_out)\n",
        "\n",
        "      return x\n",
        "\n",
        "  # testing\n",
        "n_heads = 4\n",
        "d_model = 8\n",
        "dropout = 0.2\n",
        "\n",
        "tgt_padding_mask = torch.tensor([[1, 1, 1, 0]])  # last token padded\n",
        "enc_padding_mask = torch.tensor([[1, 1, 1, 1, 0, 0]])\n",
        "\n",
        "enc_out = torch.rand(1,6,d_model)  # encoder output  ===>    (B,S,D)\n",
        "x = torch.rand(1,4,d_model)  # decoder hidden state  ===>  (B,T,C)\n",
        "\n",
        "decoder = Decoder(d_model,n_heads,dropout)\n",
        "out = decoder(x,enc_out,tgt_padding_mask,enc_padding_mask)\n",
        "print(\"Decoder output shape:\", out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYhRm_S9k9bf"
      },
      "source": [
        "# Model\n",
        "- Encoder: (B, S, C)\n",
        "- Decoder: (B, T, C)\n",
        "- Cross-attention: queries from decoder, keys/values from encoder\n",
        "- Output: (B, T, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocUvTyOyekoY"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,dropout,n_layers,max_len,src_vocab_size,tgt_vocab_size):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.max_len = max_len\n",
        "\n",
        "    # step 1: Token embedding\n",
        "    self.src_token_emb = nn.Embedding(src_vocab_size, d_model)  # for English\n",
        "    self.tgt_token_emb = nn.Embedding(tgt_vocab_size, d_model)  # for Urdu\n",
        "\n",
        "    # Step 2: Postional encoding\n",
        "    self.pos_enc = SinusoidalPostionalEncoding(max_len,d_model)\n",
        "\n",
        "    # Step 3: Encoder + Decoder Blocks Stack\n",
        "    self.encoder_blocks = nn.ModuleList([\n",
        "        Encoder(d_model,n_heads,dropout)\n",
        "        for _ in range(n_layers)\n",
        "    ])\n",
        "    self.decoder_blocks = nn.ModuleList([\n",
        "        Decoder(d_model,n_heads,dropout)\n",
        "        for _ in range(n_layers)\n",
        "    ])\n",
        "\n",
        "    # step 4: LayerNorm before final output projection  as we have used pre-norm in encoder and decoder block\n",
        "    self.layer_norm = LayerNorm(d_model)\n",
        "\n",
        "    # step 5 :Final Head linear layer\n",
        "    self.head = nn.Linear(d_model,tgt_vocab_size) # final Linear layer to project the Decoder hidden state to vacab_size logits afterward we apply softmax\n",
        "\n",
        "    # step 6 : Dropout\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,idx,target,target_padding_mask=None,enc_padding_mask=None):\n",
        "    \"\"\"\n",
        "    idx: source sequence (English) -> (B, S)\n",
        "    target: target sequence (Urdu) -> (B, T)\n",
        "    \"\"\"\n",
        "    #============= Encoder Block working =============\n",
        "    B,S = idx.shape  # encoder input (english) at this point idx is token(int) indices, not embeddings as it will latter be converted to embedding.\n",
        "    # step 1 : we first embedd the input\n",
        "    src_embedd = self.src_token_emb(idx)\n",
        "\n",
        "    #step 2:  we then apply positonal encoding\n",
        "    src_input = self.pos_enc(src_embedd)  # postional embedding class add the input+positional embedding\n",
        "\n",
        "    # step 3 we then execute the encoder block\n",
        "    x = src_input\n",
        "    for block in self.encoder_blocks:\n",
        "      x = block(x,\n",
        "                enc_padding_mask=enc_padding_mask,\n",
        "                mask=None)\n",
        "    enc_out = x\n",
        "\n",
        "\n",
        "    #============= Dncoder Block working =============\n",
        "\n",
        "    B,T = target.shape  # decoder input (urdu) at this point target is token(int) indices, not embeddings as it will latter be converted to embedding.\n",
        "\n",
        "    # step 1 : we first embedd the input\n",
        "    target_embedd = self.tgt_token_emb(target)\n",
        "\n",
        "    #step 2:  we then apply positonal encoding\n",
        "    trg_input = self.pos_enc(target_embedd) # postional embedding class add the input+positional embedding\n",
        "\n",
        "\n",
        "    # step 3 : we then execute the decoder block\n",
        "    y = trg_input  # Initialize decoder input with embedded target tokens + positional encoding\n",
        "    for block in self.decoder_blocks:\n",
        "      # Each decoder block receives the output of the previous block\n",
        "      # and attends to the encoder output with masks applied\n",
        "      y = block(x=y,\n",
        "                enc_out = enc_out,\n",
        "                tgt_padding_mask=target_padding_mask,\n",
        "                enc_padding_mask=enc_padding_mask,\n",
        "                mask=True) # it recieve Mask and enc_padding_mask  and target_padding_mask\n",
        "    dec_out = y\n",
        "\n",
        "    #================Final Head for output ========================\n",
        "    x = self.layer_norm(dec_out)\n",
        "    logits = self.head(dec_out)\n",
        "\n",
        "    return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h_V6iOuFhHq"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "d_model = 512  # embedding dimesnion\n",
        "n_heads = 8  # 512/8 ==> 64D per Head\n",
        "n_layers = 6  # no of encoder decoder block\n",
        "dropout = 0.25\n",
        "\n",
        "max_len = max_len  # max lenth of sequences(that model can handle)\n",
        "src_vocab_size = sp_en.get_piece_size()\n",
        "tgt_vocab_size = sp_ur.get_piece_size()\n",
        "\n",
        "model = MyModel(d_model = d_model,\n",
        "                n_heads = n_heads,\n",
        "                dropout = dropout,\n",
        "                n_layers = n_layers,\n",
        "                max_len = max_len,\n",
        "                src_vocab_size=src_vocab_size,\n",
        "                tgt_vocab_size=tgt_vocab_size).to(device)\n",
        "\n",
        "# torch.compile() is a graph-level compiler that optimizes your model for faster GPU/CPU execution by:\n",
        "# fusing small operation in single large one so it doesnt use many gpu kernel\n",
        "# it can make traing slow in start but later fast\n",
        "model = torch.compile(model)\n",
        "\n",
        "\n",
        "# def get_lr(step, d_model=512, warmup_steps=4000):\n",
        "#     step = max(step, 1)\n",
        "#     return (d_model ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "KTLCcSD9cYS7",
        "outputId": "c05b63cb-0bf4-4705-fad0-7f955fe62e5e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the positional encoding tensor from your trained model\n",
        "# we extrac pos_encoding from MyModel class Attribute(self.pos_enc) and pe attribute  from postional encoding class\n",
        "# .detach() this detaches the tensor from the computation graph. mean it stop traking its gradient\n",
        "#.cpu() we move tensor back to cpu from cuda as we cant plot or convert GPU tensors to NumPy arrays directly as plotting library work with cpu.\n",
        "pos_encoding = model.pos_enc.pe.squeeze(0).detach().cpu()  # shape: (max_len, d_model)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(pos_encoding, cmap='viridis', aspect='auto', interpolation='nearest')\n",
        "plt.xlabel(\"Embedding Dimension\")\n",
        "plt.ylabel(\"Position \")\n",
        "plt.title(\"Sinusoidal Positional Encoding (used by MyModel)\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# from the plot we can see that leftmost embedding have very high frequency and as we move across x-axis the frequency decreases\n",
        "# left most embedding dimesnion : These dimensions capture very fine-grained, local position information.\n",
        "# right most embedding dimension: These dimensions capture global, coarse-grained position information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOEHFpgelhLV"
      },
      "source": [
        "# Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHChZeeKliRa",
        "outputId": "de929514-1298-4404-d972-3a5b3ef7e008"
      },
      "outputs": [],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37Ru9xfcllDm"
      },
      "outputs": [],
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"{name:40s} | {param.numel():,} params | Trainable: {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRJqaNsuei7x"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.memory_allocated() / 1e9)  # in GB\n",
        "print(torch.cuda.memory_reserved() / 1e9)   # in GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3MtcOJxG16C"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='training_logs.txt',   # File where logs will be saved\n",
        "    filemode='a',                   # 'w' to overwrite, 'a' to append\n",
        "    format='%(asctime)s - %(message)s',\n",
        "    level=logging.INFO\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro3ducKQkumy"
      },
      "source": [
        "# Training Pipline\n",
        "- Given all previous words, predict the next one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl9y_5xQZAWl"
      },
      "source": [
        "# Belu intraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JFDUwDMxlZF"
      },
      "outputs": [],
      "source": [
        "import tqdm as tqdm\n",
        "\n",
        "@torch.no_grad()\n",
        "def loss_calculation(val_loader):\n",
        "    model.eval()\n",
        "    losses = {}\n",
        "    pad_token_id = sp_ur.pad_id()\n",
        "\n",
        "    for split, loader in [(\"val\", val_loader)]:\n",
        "        split_losses = []\n",
        "        for xb, yb, src_mask, tgt_mask in tqdm(loader,desc=f\"Calculating loss for {split}\"):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
        "\n",
        "            # Apply shifting\n",
        "            decoder_input = yb[:, :-1]\n",
        "            target_output = yb[:, 1:]\n",
        "            tgt_mask_shifted = tgt_mask[:, :-1]\n",
        "\n",
        "            logits = model(\n",
        "                idx=xb,\n",
        "                target=decoder_input,\n",
        "                target_padding_mask=tgt_mask_shifted,\n",
        "                enc_padding_mask=src_mask)\n",
        "\n",
        "\n",
        "            # Calculate loss\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.reshape(-1, C)  # cross entropy take (B*T,C)\n",
        "            target_flat = target_output.reshape(-1) # it is not contigous\n",
        "            loss = F.cross_entropy(logits_flat, target_flat, ignore_index=pad_token_id)  # padding will be ignored during loss calculation   as it make loss unrealistic vers small\n",
        "            split_losses.append(loss.item())\n",
        "\n",
        "        losses[split] = sum(split_losses) / len(split_losses)\n",
        "\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoKxxG2zSrLP",
        "outputId": "f095c9ca-a662-47a1-cf36-f0ec162291b4"
      },
      "outputs": [],
      "source": [
        "!pip install -qq sacrebleu bitsandbytes\n",
        "\n",
        "def translate_sentence(sentence,model,sp_en,sp_ur,max_len):\n",
        "  model.eval()\n",
        "\n",
        "  # Encode input\n",
        "  src_token = sp_en.encode(sentence,out_type=int)  # encode the english sentence\n",
        "  src = torch.tensor(src_token,dtype=torch.long).unsqueeze(0).to(device)\n",
        "  src_mask = torch.ones_like(src).to(device)  #creates a tensor of all ones with the same shape as src. There’s no padding yet because you have only one sentence, so all tokens are valid.\n",
        "\n",
        "  bos_id = sp_ur.bos_id()  #begining of sequence id\n",
        "  eos_id = sp_ur.eos_id()  # end of sequence id\n",
        "  pad_id = sp_ur.pad_id()  # padding id\n",
        "\n",
        "\n",
        "\n",
        "  # start with begining of sequnece as output\n",
        "  tgt = torch.tensor([[bos_id]],dtype=torch.long).to(device)\n",
        "  generated_token = [] #store the generated token in list\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(max_len):\n",
        "      tgt_mask = torch.ones_like(tgt).to(device)\n",
        "      logits = model(\n",
        "              idx=src,\n",
        "              target=tgt,\n",
        "              target_padding_mask=tgt_mask,\n",
        "              enc_padding_mask=src_mask)\n",
        "      if torch.isnan(logits).any():  # if their is nan in logits\n",
        "        break\n",
        "\n",
        "      # Get next token after bos\n",
        "      next_token_logit = logits[:,-1,:] # get the alst new generated token\n",
        "      probs = F.softmax(next_token_logit,dim=-1)\n",
        "\n",
        "\n",
        "      next_token = torch.argmax(probs,dim=-1,keepdim=True)\n",
        "      next_token_id = next_token.item()\n",
        "\n",
        "      generated_token.append(next_token_id)\n",
        "\n",
        "      # if the next token is eos we break the loop\n",
        "      if next_token_id == eos_id:\n",
        "        break\n",
        "      tgt = torch.cat([tgt,next_token],dim=1)\n",
        "\n",
        "\n",
        "  # Remove special tokens(eos,bos,bos_id,,pad_id)\n",
        "  pred_token = [t for t in generated_token if t not in [bos_id, eos_id, pad_id, 0]]\n",
        "\n",
        "  if len(pred_token) ==0:\n",
        "    return \"Nothing Generated\"\n",
        "\n",
        "  translation = sp_ur.decode(pred_token) # conver token to urdu\n",
        "  return translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI9ywXW0oaVz"
      },
      "source": [
        "## Lerning rate schedule\n",
        "Start small → to avoid exploding gradients\n",
        "\n",
        "Increase gradually (warmup) → so model begins to learn\n",
        "\n",
        "Decrease slowly (decay) → for fine convergence at the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2px1UD3PZK-1",
        "outputId": "1cb4b5ad-1197-48ea-eea3-52f4b4fdc9ef"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sacrebleu import corpus_bleu\n",
        "import time\n",
        "import json\n",
        "\n",
        "\n",
        "#==================== List to store values ===============\n",
        "time_per_epoch = [] #store time taken by epochs\n",
        "bleu_scores = []  #store belu scores\n",
        "train_losses = [] #store train loso\n",
        "val_losses = []  #store val loss\n",
        "lr_scheduler = [] #store all lerning rates\n",
        "\n",
        "\n",
        "# =========================Some Parameters =====================\n",
        "eval_interval = 1  # eval once per epoch\n",
        "accum_steps = 4\n",
        "epochs = 4\n",
        "pad_token_id = sp_ur.pad_id()  # get padding id for target tokens\n",
        "\n",
        "\n",
        "\n",
        "#================= Learning rate schedule ==========================\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=0.01)\n",
        "# LR increases linearly for num_warmup_steps\n",
        "# Then decreases linearly to 0 till training ends.\n",
        "num_training_steps = len(train_loader) * epochs # e.g 1508 * 10 ==> 15080 stpes\n",
        "num_warmup_steps = int(0.1 * num_training_steps)  # (10% warmup)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = num_warmup_steps, # step for increasing learning rate(warmup)\n",
        "    num_training_steps = num_training_steps # total training stpes after it decrease lr\n",
        ")\n",
        "\n",
        "\n",
        "best_bleu = 0.0\n",
        "#============================= Training Loope ==========================\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()  # clear gradient\n",
        "  total_train_loss = 0 # inital training loss\n",
        "  start_time = time.time()\n",
        "\n",
        "  for step ,(xb,yb,src_mask,tgt_mask) in enumerate(tqdm(train_loader)): # as collate_fn function return 4 things\n",
        "      xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "      src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
        "\n",
        "\n",
        "      # Step 1 : Teacher forcing for Decoder input\n",
        "      # y = [BOS, w1, w2, EOS, PAD, PAD]\n",
        "      # decoder_input =  [BOS, w1, w2, EOS, PAD]   length=5    all except the last token(EOS)\n",
        "      # target_output =  [w1, w2, EOS, PAD, PAD]   length=4   all except the first token(BOS)\n",
        "      decoder_input = yb[:,:-1]   #(batch_size,seq_len)   Urdu sequence shifted right by one\n",
        "      target_output = yb[:,1:]   #(batch_size,seq_len)    Urdu token shifted left by one.\n",
        "\n",
        "      # Original mask: [1, 1, 1, 1, 0, 0]   length = 6\n",
        "      # Remove last element → [1, 1, 1, 1, 0]  length = 5\n",
        "      # Now the mask aligns with decoder_input\n",
        "      # We remove the last token of the mask to align it with the decoder input\n",
        "      tgt_mask_shifted = tgt_mask[:,:-1]  #padding mask for the decoder input.\n",
        "\n",
        "\n",
        "      # Step 2: forward Propogation\n",
        "      logits = model(\n",
        "            idx=xb, #input sequence (source)\n",
        "            target=decoder_input, #decoder input sequence (teacher forcing)\n",
        "            target_padding_mask=tgt_mask_shifted, # padding mask for the decoder input\n",
        "            enc_padding_mask=src_mask) # padding mask for the encoder input\n",
        "\n",
        "\n",
        "\n",
        "      #Step 3 calculate loss\n",
        "      B,T,C = logits.shape\n",
        "      logits_flat = logits.reshape(-1,C)  # as cross entopy take (B,T,C) ==> (B*T,C)\n",
        "      target_flat = target_output.reshape(-1)\n",
        "      loss = F.cross_entropy(logits_flat, target_flat, ignore_index=pad_token_id) # we ignore padding in loss calculation\n",
        "      loss = loss/accum_steps\n",
        "      loss.backward()\n",
        "\n",
        "\n",
        "\n",
        "      #Step 4: Update weights after accumulation\n",
        "      if (step+1) % accum_steps ==0 or (step+1)==len(train_loader):\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # help in reducing exploding gradient issue by normalizing them\n",
        "          optimizer.step()\n",
        "          scheduler.step()  #learning rate scheduler\n",
        "          lr_scheduler.append(scheduler.get_last_lr()[0])\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "      # beacuase we divide the loss by accum_Steps it become 4 time smaller to bring it back to normal scale we multiply by accum_stpes\n",
        "      total_train_loss = total_train_loss + loss.item() * accum_steps # calculate total loss\n",
        "  avg_train_loss = total_train_loss/len(train_loader)\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "\n",
        "\n",
        "      #Step 5: Evaluation\n",
        "  if (epoch + 1) % eval_interval == 0:\n",
        "      losses = loss_calculation(val_loader)\n",
        "      # train_losses.append(losses[\"train\"])\n",
        "      val_losses.append(losses[\"val\"])\n",
        "\n",
        "      end_time = time.time()  # end time\n",
        "      epoch_time = end_time - start_time\n",
        "      time_per_epoch.append(epoch_time)\n",
        "      print(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f},val_loss={losses['val']:.4f}\")\n",
        "      logging.info(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, val_loss={losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "      #=================== BELU score calculation(val_data) ===========\n",
        "      model.eval()\n",
        "      urdu_predictions,urdu_references = [],[]\n",
        "      with torch.no_grad():\n",
        "        for src_batch,tgt_batch,src_mask,tgt_mask in tqdm(val_loader,desc=\"Evaluation BELU\"):\n",
        "          src_batch,src_mask = src_batch.to(device),src_mask.to(device)\n",
        "\n",
        "          # Dcode each sentence in the batch so we can pass raw input to our translate sentence as it handle raw sentence\n",
        "          # we only use 5 inputs per batch to speed up training\n",
        "          for src,tgt in zip(src_batch[:1],tgt_batch[:1]):\n",
        "            english_sentences = sp_en.decode(list(src.cpu().tolist()))\n",
        "            ref_sentences = sp_ur.decode(list(tgt.cpu().tolist()))\n",
        "\n",
        "            pred_sentences = translate_sentence(english_sentences, model, sp_en, sp_ur, max_len=max_len)\n",
        "            urdu_predictions.append(pred_sentences)\n",
        "            urdu_references.append(ref_sentences)\n",
        "\n",
        "        bleu = corpus_bleu(urdu_predictions, [urdu_references])\n",
        "        print(f\"Epoch {epoch+1}: BLEU score = {bleu.score:.2f}\")\n",
        "        logging.info(f\"Epoch {epoch+1}: BLEU score = {bleu.score:.2f}\")\n",
        "\n",
        "\n",
        "      # To save model weith best belu scores\n",
        "      if bleu.score > best_bleu:\n",
        "        best_bleu = bleu.score\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(f\" New best BLEU: {best_bleu:.2f}, model saved!\")\n",
        "        logging.info(f\"New best BLEU: {best_bleu:.2f}, model saved!\")\n",
        "\n",
        "       #===== ============ Save belu to List and also save that list to json file ====================\n",
        "      # Each epoch, you append(bleu.score) to this list.\n",
        "      # When you json.dump(bleu_scores, f), you are writing the entire Python list to the JSON file.\n",
        "      # This means the JSON file always contains a list of all BLEU scores so far, not a list of lists.\n",
        "      bleu_scores.append(bleu.score)  #belu scores\n",
        "      # with open(\"bleu_scores.json\", \"w\") as f:\n",
        "      #   json.dump(bleu_scores, f)  # it will store as python list inside json as we are saving what is being appended in lis we are saving the list\n",
        "\n",
        "\n",
        "      # Save losses and metrics to one JSON file so we later load them if we restart the checkpoint\n",
        "      metrics = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"time_per_epoch\": time_per_epoch,\n",
        "        \"bleu_scores\": bleu_scores,\n",
        "        \"lr_scheduler\": lr_scheduler}\n",
        "      with open(\"metrics.json\",\"w\") as f:\n",
        "        json.dump(metrics,f)\n",
        "\n",
        "\n",
        "\n",
        "      # Save checkpoint\n",
        "      torch.save({\n",
        "          'epoch': epoch,  #Keeps track of which epoch you were on when saved\n",
        "          'model_state_dict': model.state_dict(),  #Stores the model's learned parameters (weights & biases)\n",
        "          'optimizer_state_dict': optimizer.state_dict(),  # Stores optimizer’s internal state includes learning rates, momentum, Adam’s running averages (m, v), etc.\n",
        "          'scheduler_state_dict': scheduler.state_dict(),  # save scheduler state\n",
        "          'loss': loss.item() * accum_steps,}, # Saves the most recent loss value\n",
        "          f\"checkpoint_epoch_{epoch+1}.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOAOz7uGsFUf"
      },
      "source": [
        "# Resume after training interrupt(Checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rw_G_Jc3bCc",
        "outputId": "ff2ac8c5-8abf-4d69-b6a7-8694e470623c"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sacrebleu import corpus_bleu\n",
        "import time\n",
        "import json\n",
        "\n",
        "# ================== Load Previous Metris score =============================\n",
        "with open(\"metrics.json\", \"r\") as f:\n",
        "    metrics = json.load(f)\n",
        "\n",
        "train_losses = metrics[\"train_losses\"]\n",
        "val_losses = metrics[\"val_losses\"]\n",
        "time_per_epoch = metrics[\"time_per_epoch\"]\n",
        "bleu_scores = metrics[\"bleu_scores\"]\n",
        "lr_scheduler = metrics[\"lr_scheduler\"]\n",
        "\n",
        "# ================= Parameter =====================\n",
        "\n",
        "eval_interval = 1  # eval once per epoch\n",
        "accum_steps = 4\n",
        "epochs = 12\n",
        "pad_token_id = sp_ur.pad_id()  # get padding id for target tokens\n",
        "\n",
        "\n",
        "#================= Learning rate schedule ==========================\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=0.01)\n",
        "# LR increases linearly for num_warmup_steps\n",
        "# Then decreases linearly to 0 till training ends.\n",
        "num_training_steps = len(train_loader) * epochs # e.g 1508 * 10 ==> 15080 stpes\n",
        "num_warmup_steps = int(0.1 * num_training_steps)  # (10% warmup)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = num_warmup_steps, # step for increasing learning rate(warmup)\n",
        "    num_training_steps = num_training_steps # total training stpes after it decrease lr\n",
        ")\n",
        "\n",
        "\n",
        "best_bleu = max(metrics[\"bleu_scores\"]) if metrics[\"bleu_scores\"] else 0.0  # load best belu score from metrics.json\n",
        "\n",
        "\n",
        "\n",
        "# ============================= load State ================================\n",
        "# step 1 : Load checpoint\n",
        "# Reload checkpoint\n",
        "checkpoint = torch.load(\"/content/checkpoint_epoch_5.pt\",map_location=device)  # it will start from epoch 3\n",
        "# Load model weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# Load optimizer (only needed if you plan to resume training)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#Load scheduler\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "\n",
        "\n",
        "#============================= Training Loope ==========================\n",
        "\n",
        "print(f\"Resuming training from epoch {start_epoch}/{epochs}\")\n",
        "logging.info(f\"Resuming training from epoch {start_epoch}/{epochs}\")\n",
        "for epoch in range(start_epoch,epochs):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()  # clear gradient\n",
        "  total_train_loss = 0 # inital training loss\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  for step ,(xb,yb,src_mask,tgt_mask) in enumerate(tqdm(train_loader)): # as collate_fn function return 4 things\n",
        "      xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "      src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
        "\n",
        "      # Step 1 : Teacher forcing for Decoder input\n",
        "      decoder_input = yb[:,:-1]   #(batch_size,seq_len)   Urdu sequence shifted right by one\n",
        "      target_output = yb[:,1:]   #(batch_size,seq_len)    Urdu token shifted left by one.\n",
        "      # Original mask: [1, 1, 1, 1, 0, 0]   length = 6\n",
        "      tgt_mask_shifted = tgt_mask[:,:-1]  #padding mask for the decoder input.\n",
        "\n",
        "      # Step 2: forward Propogation\n",
        "      logits = model(\n",
        "            idx=xb, #input sequence (source)\n",
        "            target=decoder_input, #decoder input sequence (teacher forcing)\n",
        "            target_padding_mask=tgt_mask_shifted, # padding mask for the decoder input\n",
        "            enc_padding_mask=src_mask) # padding mask for the encoder input\n",
        "\n",
        "      #Step 3 calculate loss\n",
        "      B,T,C = logits.shape\n",
        "      logits_flat = logits.reshape(-1,C)  # as cross entopy take (B,T,C) ==> (B*T,C)\n",
        "      target_flat = target_output.reshape(-1)\n",
        "      loss = F.cross_entropy(logits_flat, target_flat, ignore_index=pad_token_id) # we ignore padding in loss calculation\n",
        "      loss = loss/accum_steps\n",
        "      loss.backward()\n",
        "\n",
        "      #Step 4: Update weights after accumulation\n",
        "      if (step+1) % accum_steps ==0 or (step+1)==len(train_loader):\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # help in reducing exploding gradient issue by normalizing them\n",
        "          optimizer.step()\n",
        "          scheduler.step()  #learning rate scheduler\n",
        "          lr_scheduler.append(scheduler.get_last_lr()[0])\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "      total_train_loss = total_train_loss + loss.item() * accum_steps # calculate total loss\n",
        "  avg_train_loss = total_train_loss/len(train_loader)\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "      #Step 5: Evaluation\n",
        "  if (epoch + 1) % eval_interval == 0:\n",
        "      losses = loss_calculation(val_loader)\n",
        "      # train_losses.append(losses[\"train\"])\n",
        "      val_losses.append(losses[\"val\"])\n",
        "\n",
        "      end_time = time.time()  # end time\n",
        "      epoch_time = end_time - start_time\n",
        "      time_per_epoch.append(epoch_time)\n",
        "      print(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f},val_loss={losses['val']:.4f}\")\n",
        "      logging.info(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, val_loss={losses['val']:.4f}\")\n",
        "\n",
        "      #=================== BELU score calculation(val_data) ===========\n",
        "      model.eval()\n",
        "      urdu_predictions,urdu_references = [],[]\n",
        "      with torch.no_grad():\n",
        "        for src_batch,tgt_batch,src_mask,tgt_mask in tqdm(val_loader,desc=\"Evaluation BELU\"):\n",
        "          src_batch,src_mask = src_batch.to(device),src_mask.to(device)\n",
        "\n",
        "          # Dcode each sentence in the batch so we can pass raw input to our translate sentence as it handle raw sentence\n",
        "          # we only use 5 inputs per batch to speed up training\n",
        "          for src,tgt in zip(src_batch[:1],tgt_batch[:1]):\n",
        "            english_sentences = sp_en.decode(list(src.cpu().tolist()))\n",
        "            ref_sentences = sp_ur.decode(list(tgt.cpu().tolist()))\n",
        "\n",
        "            pred_sentences = translate_sentence(english_sentences, model, sp_en, sp_ur, max_len=max_len)\n",
        "            urdu_predictions.append(pred_sentences)\n",
        "            urdu_references.append(ref_sentences)\n",
        "\n",
        "        bleu = corpus_bleu(urdu_predictions, [urdu_references])\n",
        "        print(f\"Epoch {epoch+1}: BLEU score = {bleu.score:.2f}\")\n",
        "        logging.info(f\"Epoch {epoch+1}: BLEU score = {bleu.score:.2f}\")\n",
        "        bleu_scores.append(bleu.score)\n",
        "\n",
        "\n",
        "      # To save model weith best belu scores\n",
        "      if bleu.score > best_bleu:\n",
        "        best_bleu = bleu.score\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(f\" New best BLEU: {best_bleu:.2f}, model saved!\")\n",
        "        logging.info(f\"New best BLEU: {best_bleu:.2f}, model saved!\")\n",
        "\n",
        "       #===== ============ Save Metrics to file ====================\n",
        "\n",
        "       # Save losses and metrics to one JSON file so we later load them if we restart the checkpoint\n",
        "      metrics = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"time_per_epoch\": time_per_epoch,\n",
        "        \"bleu_scores\": bleu_scores,\n",
        "        \"lr_scheduler\": lr_scheduler}\n",
        "      with open(\"metrics.json\",\"w\") as f:\n",
        "        json.dump(metrics,f)\n",
        "\n",
        "      # Save checkpoint\n",
        "      torch.save({\n",
        "          'epoch': epoch,  #Keeps track of which epoch you were on when saved\n",
        "          'model_state_dict': model.state_dict(),  #Stores the model's learned parameters (weights & biases)\n",
        "          'optimizer_state_dict': optimizer.state_dict(),  # Stores optimizer’s internal state includes learning rates, momentum, Adam’s running averages (m, v), etc.\n",
        "          'scheduler_state_dict': scheduler.state_dict(),  # save scheduler state\n",
        "          'loss': loss.item() * accum_steps,}, # Saves the most recent loss value\n",
        "          f\"checkpoint_epoch_{epoch+1}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FJUVgC0-cKo",
        "outputId": "9d849f35-e15d-438a-af39-28ac0ecfe066"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sacrebleu import corpus_bleu\n",
        "import time\n",
        "import json\n",
        "\n",
        "# ================== Load Previous Metris score =============================\n",
        "with open(\"metrics.json\", \"r\") as f:\n",
        "    metrics = json.load(f)\n",
        "\n",
        "train_losses = metrics[\"train_losses\"]\n",
        "val_losses = metrics[\"val_losses\"]\n",
        "time_per_epoch = metrics[\"time_per_epoch\"]\n",
        "bleu_scores = metrics[\"bleu_scores\"]\n",
        "lr_scheduler = metrics[\"lr_scheduler\"]\n",
        "\n",
        "# ================= Parameter =====================\n",
        "\n",
        "eval_interval = 1  # eval once per epoch\n",
        "accum_steps = 4\n",
        "epochs = 20\n",
        "pad_token_id = sp_ur.pad_id()  # get padding id for target tokens\n",
        "\n",
        "\n",
        "#================= Learning rate schedule ==========================\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=0.01)\n",
        "# LR increases linearly for num_warmup_steps\n",
        "# Then decreases linearly to 0 till training ends.\n",
        "num_training_steps = len(train_loader) * epochs # e.g 1508 * 10 ==> 15080 stpes\n",
        "num_warmup_steps = int(0.1 * num_training_steps)  # (10% warmup)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = num_warmup_steps, # step for increasing learning rate(warmup)\n",
        "    num_training_steps = num_training_steps # total training stpes after it decrease lr\n",
        ")\n",
        "\n",
        "\n",
        "best_bleu = max(metrics[\"bleu_scores\"]) if metrics[\"bleu_scores\"] else 0.0  # load best belu score from metrics.json\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================= load State ================================\n",
        "# step 1 : Load checpoint\n",
        "# Reload checkpoint\n",
        "checkpoint = torch.load(\"/content/checkpoint_epoch_12.pt\",map_location=device)  # it will start from epoch 12\n",
        "# Load model weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# Load optimizer (only needed if you plan to resume training)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#Load scheduler\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "\n",
        "\n",
        "#============================= Training Loop ====================\n",
        "\n",
        "print(f\"Resuming training from epoch {start_epoch}/{epochs}\")\n",
        "for epoch in range(start_epoch,epochs):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()  # clear gradient\n",
        "  total_train_loss = 0 # inital training loss\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  for step ,(xb,yb,src_mask,tgt_mask) in enumerate(tqdm(train_loader)): # as collate_fn function return 4 things\n",
        "      xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "      src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
        "\n",
        "      # Step 1 : Teacher forcing for Decoder input\n",
        "      decoder_input = yb[:,:-1]   #(batch_size,seq_len)   Urdu sequence shifted right by one\n",
        "      target_output = yb[:,1:]   #(batch_size,seq_len)    Urdu token shifted left by one.\n",
        "      # Original mask: [1, 1, 1, 1, 0, 0]   length = 6\n",
        "      tgt_mask_shifted = tgt_mask[:,:-1]  #padding mask for the decoder input.\n",
        "\n",
        "      # Step 2: forward Propogation\n",
        "      logits = model(\n",
        "            idx=xb, #input sequence (source)\n",
        "            target=decoder_input, #decoder input sequence (teacher forcing)\n",
        "            target_padding_mask=tgt_mask_shifted, # padding mask for the decoder input\n",
        "            enc_padding_mask=src_mask) # padding mask for the encoder input\n",
        "\n",
        "      #Step 3 calculate loss\n",
        "      B,T,C = logits.shape\n",
        "      logits_flat = logits.reshape(-1,C)  # as cross entopy take (B,T,C) ==> (B*T,C)\n",
        "      target_flat = target_output.reshape(-1)\n",
        "      loss = F.cross_entropy(logits_flat, target_flat, ignore_index=pad_token_id) # we ignore padding in loss calculation\n",
        "      loss = loss/accum_steps\n",
        "      loss.backward()\n",
        "\n",
        "      #Step 4: Update weights after accumulation\n",
        "      if (step+1) % accum_steps ==0 or (step+1)==len(train_loader):\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)   # help in reducing exploding gradient issue by normalizing them\n",
        "          optimizer.step()\n",
        "          scheduler.step()  #learning rate scheduler\n",
        "          lr_scheduler.append(scheduler.get_last_lr()[0])\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "      total_train_loss = total_train_loss + loss.item() * accum_steps # calculate total loss\n",
        "  avg_train_loss = total_train_loss/len(train_loader)\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "      #Step 5: Evaluation\n",
        "  if (epoch + 1) % eval_interval == 0:\n",
        "      losses = loss_calculation(val_loader)\n",
        "      # train_losses.append(losses[\"train\"])\n",
        "      val_losses.append(losses[\"val\"])\n",
        "\n",
        "      end_time = time.time()  # end time\n",
        "      epoch_time = end_time - start_time\n",
        "      time_per_epoch.append(epoch_time)\n",
        "      print(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f},val_loss={losses['val']:.4f}\")\n",
        "\n",
        "      #=================== BELU score calculation(val_data) ===========\n",
        "      model.eval()\n",
        "      urdu_predictions,urdu_references = [],[]\n",
        "      with torch.no_grad():\n",
        "        for src_batch,tgt_batch,src_mask,tgt_mask in tqdm(val_loader,desc=\"Evaluation BELU\"):\n",
        "          src_batch,src_mask = src_batch.to(device),src_mask.to(device)\n",
        "\n",
        "          # Dcode each sentence in the batch so we can pass raw input to our translate sentence as it handle raw sentence\n",
        "          # we only use 5 inputs per batch to speed up training\n",
        "          for src,tgt in zip(src_batch[:1],tgt_batch[:1]):\n",
        "            english_sentences = sp_en.decode(list(src.cpu().tolist()))\n",
        "            ref_sentences = sp_ur.decode(list(tgt.cpu().tolist()))\n",
        "\n",
        "            pred_sentences = translate_sentence(english_sentences, model, sp_en, sp_ur, max_len=max_len)\n",
        "            urdu_predictions.append(pred_sentences)\n",
        "            urdu_references.append(ref_sentences)\n",
        "\n",
        "        bleu = corpus_bleu(urdu_predictions, [urdu_references])\n",
        "        print(f\"Epoch {epoch+1}: BLEU score = {bleu.score:.2f}\")\n",
        "        bleu_scores.append(bleu.score)\n",
        "\n",
        "\n",
        "      # To save model weith best belu scores\n",
        "      if bleu.score > best_bleu:\n",
        "        best_bleu = bleu.score\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(f\" New best BLEU: {best_bleu:.2f}, model saved!\")\n",
        "\n",
        "       #===== ============ Save Metrics to file ====================\n",
        "\n",
        "       # Save losses and metrics to one JSON file so we later load them if we restart the checkpoint\n",
        "      metrics = {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"time_per_epoch\": time_per_epoch,\n",
        "        \"bleu_scores\": bleu_scores,\n",
        "        \"lr_scheduler\": lr_scheduler}\n",
        "      with open(\"metrics.json\",\"w\") as f:\n",
        "        json.dump(metrics,f)\n",
        "\n",
        "      # Save checkpoint\n",
        "      torch.save({\n",
        "          'epoch': epoch,  #Keeps track of which epoch you were on when saved\n",
        "          'model_state_dict': model.state_dict(),  #Stores the model's learned parameters (weights & biases)\n",
        "          'optimizer_state_dict': optimizer.state_dict(),  # Stores optimizer’s internal state includes learning rates, momentum, Adam’s running averages (m, v), etc.\n",
        "          'scheduler_state_dict': scheduler.state_dict(),  # save scheduler state\n",
        "          'loss': loss.item() * accum_steps,}, # Saves the most recent loss value\n",
        "          f\"checkpoint_epoch_{epoch+1}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJTBeN6svbkL"
      },
      "outputs": [],
      "source": [
        "# save file to drive\n",
        "!mkdir -p /content/drive/MyDrive/MachineTranslation  #create a directery\n",
        "\n",
        "!cp /content/checkpoint_epoch_20.pt /content/drive/MyDrive/MachineTranslation/\n",
        "!cp metrics.json /content/drive/MyDrive/MachineTranslation/\n",
        "!cp /content/urdu.model /content/drive/MyDrive/MachineTranslation/\n",
        "!cp /content/english.model /content/drive/MyDrive/MachineTranslation/\n",
        "!cp /content/merge_data.csv /content/drive/MyDrive/MachineTranslation/\n",
        "!cp /content/best_model.pt /content/drive/MyDrive/MachineTranslation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsS8XupSFrEQ"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "q4h0ETBtFsg7",
        "outputId": "b0c10879-7329-421f-fee3-47ecb9490b3a"
      },
      "outputs": [],
      "source": [
        "figure,axis = plt.subplots(2,2,figsize=(12,7))\n",
        "axis = axis.flatten()\n",
        "\n",
        "epoch_range_time = range(1,len(time_per_epoch)+1)\n",
        "axis[0].plot(epoch_range_time,time_per_epoch,marker=\"s\",color=\"green\")\n",
        "axis[0].grid()\n",
        "axis[0].set_title(\"Time take Per Epochs\")\n",
        "axis[0].set_xlabel(\"Epochs\")\n",
        "axis[0].set_ylabel(\"Time Taken\")\n",
        "\n",
        "# belu score per\n",
        "x = range(1,len(bleu_scores)+1)\n",
        "axis[1].plot(x,bleu_scores,marker=\"s\",color=\"blue\")\n",
        "axis[1].set_title(\"BELU vs Epochs\")\n",
        "axis[1].set_xlabel(\"Epoch\")\n",
        "axis[1].set_ylabel(\"BLEU Score\")\n",
        "axis[1].grid(True)\n",
        "\n",
        "\n",
        "# learning rate scheduler plot\n",
        "axis[2].plot(lr_scheduler)\n",
        "axis[2].set_title(\"Learning rate schedule(warmup + linear decay)\")\n",
        "axis[2].set_xlabel(\"steps\")\n",
        "axis[2].set_ylabel(\"Learning rate\")\n",
        "axis[2].grid()\n",
        "\n",
        "\n",
        "# train vs val loss over epochs loss plot\n",
        "epochs_range_train_val = range(1, len(train_losses) + 1) # total loss == total epochs\n",
        "# epochs_range = range(1,2)\n",
        "axis[3].plot(epochs_range_train_val,train_losses,label=\"Train losses\",marker = \"o\")\n",
        "axis[3].plot(epochs_range_train_val,val_losses,label=\"Val Losses\",marker = \"s\")\n",
        "axis[3].set_ylabel(\"Loss\")\n",
        "axis[3].set_xlabel(\"Epochs\")\n",
        "axis[3].set_title(\"Train vs Val Loss\")\n",
        "axis[3].legend()\n",
        "axis[3].grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIlouUe6cd29"
      },
      "source": [
        "## Downlaod file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZtGoexFcc6Q"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/checkpoint_epoch_4.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV61m-FVp1zR"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciERa9h6NqZu",
        "outputId": "f0fa8689-19d8-461a-8b30-2a69220f00e0"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence,model,sp_en,sp_ur,max_len):\n",
        "  model.eval()\n",
        "\n",
        "  # Encode input\n",
        "  src_token = sp_en.encode(sentence,out_type=int)  # encode the english sentence\n",
        "  src = torch.tensor(src_token,dtype=torch.long).unsqueeze(0).to(device)\n",
        "  src_mask = torch.ones_like(src).to(device)  #creates a tensor of all ones with the same shape as src. There’s no padding yet because you have only one sentence, so all tokens are valid.\n",
        "\n",
        "  bos_id = sp_ur.bos_id()  #begining of sequence id\n",
        "  eos_id = sp_ur.eos_id()  # end of sequence id\n",
        "  pad_id = sp_ur.pad_id()  # padding id\n",
        "  print(f\"BOS: {bos_id}, EOS: {eos_id}, PAD: {pad_id}\")\n",
        "\n",
        "\n",
        "  # start with begining of sequnece as output\n",
        "  tgt = torch.tensor([[bos_id]],dtype=torch.long).to(device)\n",
        "  generated_token = [] #store the generated token in list\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(max_len):\n",
        "      tgt_mask = torch.ones_like(tgt).to(device)\n",
        "      logits = model(\n",
        "              idx=src,\n",
        "              target=tgt,\n",
        "              target_padding_mask=tgt_mask,\n",
        "              enc_padding_mask=src_mask)\n",
        "      if torch.isnan(logits).any():  # if their is nan in logits\n",
        "        print(f\"Nan at step {i}\")\n",
        "        break\n",
        "\n",
        "      # Get next token after bos\n",
        "      next_token_logit = logits[:,-1,:] # get the alst new generated token\n",
        "      probs = F.softmax(next_token_logit,dim=-1)\n",
        "\n",
        "      # Debug\n",
        "      if i < 3:\n",
        "        top5_probs, top5_ids = torch.topk(probs,k=5)  # we get top 5 probability from the probs for the token to be generated\n",
        "        print(f\"\\nstep {i}:\")\n",
        "        for prob,idx in zip(top5_probs[0],top5_ids[0]):\n",
        "          print(f\"{sp_ur.id_to_piece(idx.item())}: {prob.item():.4f}\")  # we extract ids and their probability\n",
        "\n",
        "      next_token = torch.argmax(probs,dim=-1,keepdim=True)\n",
        "      next_token_id = next_token.item()\n",
        "\n",
        "      generated_token.append(next_token_id)\n",
        "\n",
        "      # if the next token is eos we break the loop\n",
        "      if next_token_id == eos_id:\n",
        "        print(f\"EOS at step {i}\")\n",
        "        break\n",
        "      tgt = torch.cat([tgt,next_token],dim=1)\n",
        "\n",
        "  print(f\"Generated: {generated_token}\")\n",
        "\n",
        "  # Remove special tokens(eos,bos,bos_id,,pad_id)\n",
        "  pred_token = [t for t in generated_token if t not in [bos_id, eos_id, pad_id, 0]]\n",
        "\n",
        "  if len(pred_token) ==0:\n",
        "    return \"Nothing Generated\"\n",
        "\n",
        "  translation = sp_ur.decode(pred_token) # conver token to urdu\n",
        "  return translation\n",
        "\n",
        "\n",
        "test_sentences = [\n",
        "    \"How are you?\",\n",
        "    \"I love you.\",\n",
        "    \"How can I communicate with my parents?\t\",\n",
        "    \"What is the meaning of this?\",\n",
        "    \"Where do you live?\",\n",
        "    \"This is very fun.\"\n",
        "]\n",
        "\n",
        "for eng in test_sentences:\n",
        "    urdu = translate_sentence(eng, model, sp_en, sp_ur, max_len=max_len)\n",
        "    print(f\"\\nEN: {eng}\")\n",
        "    print(f\"UR: {urdu}\")\n",
        "    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAZUGLjtQfr5",
        "outputId": "83e4369a-e93d-4580-c11f-ecd479ef4fe8"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence,model,sp_en,sp_ur,max_len):\n",
        "  model.eval()\n",
        "\n",
        "  # Encode input\n",
        "  src_token = sp_en.encode(sentence,out_type=int)  # encode the english sentence\n",
        "  src = torch.tensor(src_token,dtype=torch.long).unsqueeze(0).to(device)\n",
        "  src_mask = torch.ones_like(src).to(device)  #creates a tensor of all ones with the same shape as src. There’s no padding yet because you have only one sentence, so all tokens are valid.\n",
        "\n",
        "  bos_id = sp_ur.bos_id()  #begining of sequence id\n",
        "  eos_id = sp_ur.eos_id()  # end of sequence id\n",
        "  pad_id = sp_ur.pad_id()  # padding id\n",
        "\n",
        "\n",
        "  # start with begining of sequnece as output\n",
        "  tgt = torch.tensor([[bos_id]],dtype=torch.long).to(device)\n",
        "  generated_token = [] #store the generated token in list\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(max_len):\n",
        "      tgt_mask = torch.ones_like(tgt).to(device)\n",
        "      logits = model(\n",
        "              idx=src,\n",
        "              target=tgt,\n",
        "              target_padding_mask=tgt_mask,\n",
        "              enc_padding_mask=src_mask)\n",
        "      if torch.isnan(logits).any():  # if their is nan in logits\n",
        "        break\n",
        "\n",
        "      # Get next token after bos\n",
        "      next_token_logit = logits[:,-1,:] # get the alst new generated token\n",
        "      probs = F.softmax(next_token_logit,dim=-1)\n",
        "\n",
        "\n",
        "      next_token = torch.argmax(probs,dim=-1,keepdim=True)\n",
        "      next_token_id = next_token.item()\n",
        "\n",
        "      generated_token.append(next_token_id)\n",
        "\n",
        "      # if the next token is eos we break the loop\n",
        "      if next_token_id == eos_id:\n",
        "        break\n",
        "      tgt = torch.cat([tgt,next_token],dim=1)\n",
        "\n",
        "\n",
        "  # Remove special tokens(eos,bos,bos_id,,pad_id)\n",
        "  pred_token = [t for t in generated_token if t not in [bos_id, eos_id, pad_id, 0]]\n",
        "\n",
        "  if len(pred_token) ==0:\n",
        "    return \"Nothing Generated\"\n",
        "\n",
        "  translation = sp_ur.decode(pred_token) # conver token to urdu\n",
        "  return translation\n",
        "\n",
        "\n",
        "test_sentences = [\n",
        "    \"How are you?\",\n",
        "    \"I love you.\",\n",
        "    \"How can I communicate with my parents?\t\",\n",
        "    \"Where do you live?\",\n",
        "    \"This is very fun.\"\n",
        "]\n",
        "\n",
        "for eng in test_sentences:\n",
        "    urdu = translate_sentence(eng, model, sp_en, sp_ur, max_len=max_len)\n",
        "    print(f\"\\nEN: {eng}\")\n",
        "    print(f\"UR: {urdu}\")\n",
        "    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaHc7HYRqqvr"
      },
      "source": [
        "### Resume training after interrupt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGGjrm62Kr23"
      },
      "source": [
        "# Make Prediction After interputing the training to see if model is good or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqE5yocMKBr7"
      },
      "outputs": [],
      "source": [
        "# Reload checkpoint\n",
        "checkpoint = torch.load(\"checkpoint_epoch_2.pt\",map_location=device)  # it will start from epoch 3\n",
        "\n",
        "# Load model weights\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Load optimizer (only needed if you plan to resume training)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#set to evaluation\n",
        "model.eval()\n",
        "\n",
        "# Translate\n",
        "eng_sentence = \"How are you?\"\n",
        "urdu_translation = translate_sentence(eng, model, sp_en, sp_ur, max_len=max_len)\n",
        "print(\"\\nEnglish:\", eng_sentence)\n",
        "print(\"Urdu:\", urdu_translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd-BBtZyyD4n"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r4ODOFHyByt"
      },
      "outputs": [],
      "source": [
        "# Save only the model weights\n",
        "torch.save(model.state_dict(), \"trained_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EfQkKmwyHJc"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN12hb4NyBxA"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize the model architecture\n",
        "model = MyModel(d_model = d_model,\n",
        "                n_heads = n_heads,\n",
        "                dropout = dropout,\n",
        "                n_layers = n_layers,\n",
        "                max_len = max_len,\n",
        "                src_vocab_size=src_vocab_size,\n",
        "                tgt_vocab_size=tgt_vocab_size).to(device)\n",
        "\n",
        "# 2. Load the saved weights\n",
        "model.load_state_dict(torch.load(\"trained_model.pt\", map_location=device))\n",
        "\n",
        "# 3. Set the model to evaluation mode (important!)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zp-T20S9TqI"
      },
      "source": [
        "# BLEU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "des1uEOQC4VM"
      },
      "source": [
        "BLEU 20–35 is decent for such a dataset and setup.\n",
        "\n",
        "BLEU 35–45 is very good — means your model captures strong translation quality.\n",
        "\n",
        "BLEU >45 is excellent, achievable only if:\n",
        "\n",
        "You trained with large vocabularies.\n",
        "\n",
        "Have good tokenization coverage (no many <unk> tokens).\n",
        "\n",
        "Model capacity and training duration are high (e.g., 20–30 epochs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "m0VyewmI1OII",
        "outputId": "ccf0d8cd-d3d2-4377-f2e1-8e963ebbf03b"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "\n",
        "sample_size = 50  # number of sentence to be evaluate\n",
        "urdu_references = []\n",
        "urdu_predictions = []\n",
        "\n",
        "bleu_scores = []  # it will have sam no of scores as out sample_size\n",
        "\n",
        "# decode full validation set from dataloader\n",
        "val_input = []\n",
        "val_target = []\n",
        "\n",
        "# we flattent it i.e we convert the batch in to individuall sequence and convert them to python list as belu require raw sentences(no batch)\n",
        "for src_batch, tgt_batch, _, _ in val_loader:\n",
        "    for src, tgt in zip(src_batch, tgt_batch):\n",
        "        en_sentence = sp_en.decode(list(src.cpu().tolist()))  # move to cpu and convert to python list\n",
        "        ur_sentence = sp_ur.decode(list(tgt.cpu().tolist()))\n",
        "        val_input.append(en_sentence)\n",
        "        val_target.append(ur_sentence)\n",
        "\n",
        "# using first 50 inputs\n",
        "for en_sentence, ur_sentence in tqdm(zip(val_input[:sample_size], val_target[:sample_size]),total = sample_size,desc=\"Evaluation BELU scores\"):\n",
        "    pred_urdu = translate_sentence(en_sentence, model, sp_en, sp_ur, max_len)\n",
        "    urdu_predictions.append(pred_urdu)\n",
        "    urdu_references.append(ur_sentence)\n",
        "\n",
        "    # sacrebleu expects list of lists for references\n",
        "    bleu = sacrebleu.corpus_bleu(urdu_predictions, [urdu_references])\n",
        "    bleu_scores.append(bleu.score)\n",
        "\n",
        "# Print final scores belu\n",
        "print(f\"\\nFinal BLEU score after {sample_size} sentences: {bleu_scores[-1]:.2f}\")\n",
        "\n",
        "#plot belu scores\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(bleu_scores)+1), bleu_scores, marker='o', color='teal')\n",
        "plt.xlabel(\"Number of Sentences Evaluated\")\n",
        "plt.ylabel(\"Cumulative BLEU Score\")\n",
        "plt.title(\"BLEU Score on Validation Set\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "aj0BZbszoFiv",
        "outputId": "5db02001-5c65-4483-e64d-17f026db7252"
      },
      "outputs": [],
      "source": [
        "# clear gpu memory full clean\n",
        "import gc\n",
        "del model,optimizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0gkavXaqxCK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
